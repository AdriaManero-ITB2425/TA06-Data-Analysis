# TA06 - Climate Data Analysis Project üåç


**Leveraging AI and Meteorological Data for Green Transformation Solutions**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

![Project Visualization](task_02/charts&graphs/my_plot1.png)

---

*In the face of urgent challenges such as climate change, loss of biodiversity,  resource constraints, emerging micropollutants or waste of water. For instance,  cities need to improve or reinvent urban services.
It is compulsory to design and deploy innovative solutions and unique technologies for managing water, waste and energy.
The objective of this task, is to use the power of data and the most advanced AI technologies trained with ‚Äúour expertise‚Äù, to tailor solutions in the green transformation, be it decarbonization, decontamination, resource saving and regeneration, or any other solution.*
It is for this reason that we set ourselves the challenge of processing the data that the AEMET ‚ÄúAgencia Estatal de Meteorolog√≠a‚Äù publishes on the web OpenData AEMET.

---

## Table of Contents

1. [Obtenci√≥n de datos](#1-obtenci√≥n-de-datos)  
2. [Organizaci√≥n y procesamiento de datos](#2-organizacion-y-procesamiento-de-datos)  
3. [An√°lisis de datos y visualizaci√≥n](#3-analisis-de-datos-y-visualizacion)  
   3.1 [An√°lisis de datos](#3.1-analisis-de-datos)  
   3.2 [Visualizaci√≥n de datos](#3.2-visualizacion-de-datos)
4. [Publicaci√≥n de datos](#4-publicacion-de-datos)  
5. [Reflexi√≥n](#5-reflexion)  
6. [Referencias](#6-referencias)  

---

## 1. Obtenci√≥n de datos

Para realizar esta tarea es necesario obtener los datos de una fuente fiable de meteorolog√≠a. En este caso, se ha optado por la **AEMET**, la Agencia Estatal de Meteorolog√≠a de Espa√±a.  

La AEMET pone a disposici√≥n de los usuarios una **API** para la obtenci√≥n de datos meteorol√≥gicos. Se ha elegido la API de datos abiertos de AEMET, la cual est√° disponible en la siguiente direcci√≥n:  

‚û°Ô∏è [https://opendata.aemet.es/centrodedescargas/altaUsuario](https://opendata.aemet.es/centrodedescargas/altaUsuario)  

---

### Pasos:  

1. **Obtener la API Key**  
   Una vez obtenida la clave de acceso (API Key), podemos acceder a los datos deseados desde:  
   ‚û°Ô∏è [https://www.aemet.es/es/serviciosclimaticos/cambio_climat/datos_diarios](https://www.aemet.es/es/serviciosclimaticos/cambio_climat/datos_diarios)  

2. **Datos seleccionados:**  
   - **M√©todo:** Regresi√≥n rejilla  
   - **Modelo:** MIROC5  
   - **Escenario:** RCP 6.0  
   - **Variable:** Precipitaci√≥n  
   - **Periodo:** 2006‚Äì2100  

3. **Descarga del archivo con los datos:**  
   Los datos se pueden descargar desde los siguientes enlaces:  
   - [Enlace 1](https://www.aemet.es/documentos_d/serviciosclimaticos/cambio_climat/datos_diarios/reg_e/ar5/sdsm_rej/MIROC5/RCP60/precip.MIROC5.RCP60.2006-2100.SDSM_REJ.tar.gz)  
   - [Enlace 2](https://www.aemet.es/documentos_d/serviciosclimaticos/cambio_climat/datos_diarios/reg_e/ar5/sdsm_rej/MIROC5/RCP60/precip.MIROC5.RCP60.2006-2100.SDSM_REJ.tar.gz)  

# 2. Organizacion y procesamiento de datos

Una vez descargado el fichero, lo descomprimimos y obtenemos los ficheros con los datos de precipitaci√≥n.

### Ejemplo de datos: 
````csv
precip	MIROC5	RCP60	REGRESION	decimas	1
P1	35.307	-2.948	182	geo	2006	2100	-1
P1 2006 1 0 0 0 24 21 20 0 0 0 0 0 20 35 37 18 0 0 0 13 14 0 0 0 0 0 0 0 0 14 0 0 
P1 2006 2 0 0 0 13 0 0 0 0 2 0 0 0 0 0 31 0 0 0 0 0 23 25 21 26 0 0 0 0 -999 -999 -999 
P1 2006 3 0 0 11 0 0 16 20 24 0 0 39 38 23 0 0 19 17 0 12 0 0 0 0 0 16 0 0 7 0 15 20 
P1 2006 4 9 0 0 0 0 7 0 0 0 15 6 0 0 0 0 0 0 0 14 0 0 0 0 0 0 15 23 0 0 19 -999 
P1 2006 5 0 14 15 16 0 0 11 0 0 0 0 0 0 0 0 0 0 0 10 6 0 0 0 0 0 0 0 0 0 0 0 
P1 2006 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 7 0 0 0 0 -999 
P1 2006 7 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 
P1 2006 8 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 4 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 
P1 2006 9 0 5 0 0 0 0 0 0 0 0 5 0 0 4 0 0 0 0 0 0 0 0 0 9 0 0 6 0 0 0 -999 
````

### Detalles de los datos  

Los datos descargados tienen la siguiente estructura:  

1. **Primera fila**:  
   Contiene informaci√≥n sobre el tipo de datos en el archivo:  
   - Variable: Precipitaci√≥n  
   - Modelo: MIROC5  
   - Escenario: RCP60  
   - M√©todo: Regresi√≥n  
   - Unidades: D√©cimas  

2. **Segunda fila**:  
   Incluye informaci√≥n de referencia:  
   - Longitud y latitud de la localizaci√≥n  
   - A√±o de inicio y fin de los datos  
   - Valor de la d√©cima  

3. **Estructura de las columnas**:  
   - **Primera columna**: ID de la estaci√≥n (e.g., `P1`).  
   - **Segunda columna**: A√±o.  
   - **Tercera columna**: Mes.  
   - **Columnas siguientes**: Valores de precipitaci√≥n para cada d√≠a del mes (hasta 31 d√≠as).  

---

### Validaci√≥n de la estructura de los ficheros  

Te√≥ricamente, todos los archivos deben seguir esta misma estructura. Sin embargo, es necesario validar que:  

1. **Cabecera uniforme**:  
   - Todos los archivos deben tener la misma primera l√≠nea de cabecera.  

2. **N√∫mero de l√≠neas y columnas**:  
   - Cada archivo debe contener **1140 l√≠neas** (12 meses √ó 95 a√±os) + 2 l√≠neas de cabecera.  
   - Cada l√≠nea debe tener **34 columnas** (ID de estaci√≥n, a√±o, mes y 31 d√≠as).  

3. **Formato de los datos**:  
   - Todas las columnas (excepto la primera y las dos filas de cabecera) deben contener valores de tipo **INT**.  

4. **Datos completos**:  
   - No deben existir datos nulos ni valores inconsistentes.  

---

### Soluci√≥n implementada  

Para garantizar la calidad de los datos y que cumplan con las condiciones anteriores, hemos desarrollado un script en Python que realiza las siguientes validaciones:  
- Verifica que la primera l√≠nea de todos los archivos sea id√©ntica.  
- Comprueba que cada archivo contenga exactamente 1140 l√≠neas y 34 columnas.  
- Valida que los valores en las columnas (excepto las filas de cabecera) sean de tipo **INT**.  

De esta manera, aseguramos la integridad y uniformidad de los datos antes de proceder con el an√°lisis.  


**task_02/Data_Validation.py**

### Breakdown del script

Este script se compone de tres funciones principales, cada una con un prop√≥sito espec√≠fico relacionado con la validaci√≥n de los datos descargados. A continuaci√≥n, se detalla el prop√≥sito y funcionamiento de cada funci√≥n.

---

### 1. **Comprobar si un string existe en un fichero**

```python
def string_exists_in_file(filepath, target_string):
    try:
        with open(filepath, 'r', encoding='utf-8') as file:
            return any(target_string in line for line in file)
    except FileNotFoundError:
        logging.error(f"The file '{filepath}' was not found.")
    except PermissionError:
        logging.error(f"Permission denied to read the file '{filepath}'.")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
    return False
```

#### **Prop√≥sito**
Revisa si un string espec√≠fico (`target_string`) est√° presente en un fichero (`filepath`).

---

#### **Uso**
Se utiliza para comprobar si la primera l√≠nea de los ficheros es correcta.

---

#### **Detalles**
- **Errores manejados:**
  - Si el fichero no existe: Se registra un error en el log.
  - Si no se tienen permisos para leer el fichero: Se registra un error en el log.
  - Si ocurre un error inesperado: Se registra el error correspondiente en el log.
---

### 2. **Comprobar las dimensiones de un fichero**

```python
def check_file_dimensions(df, filename):
    """
    Checks if the DataFrame has 1140 rows and 33 columns.
    If dimensions differ, logs an error with the actual dimensions.
    """
    rows, cols = df.shape
    if rows != 1140 or cols != 34:
        logging.error(
            f"Dimension mismatch in file '{filename}': Found {rows} rows and {cols} columns, "
            "expected 1140 rows and 33 columns."
        )
```
#### **Prop√≥sito**
Valida que las dimensiones del fichero sean las esperadas: **1140 filas** y **34 columnas**.

---

#### **Detalles**

- **Entrada**:
  - `df`: DataFrame que representa el contenido del fichero.
  - `filename`: Nombre del fichero (para registro en el log).

- **Salida**:
  - No devuelve nada. Registra un error en caso de que las dimensiones no sean correctas.

---

#### **Validaci√≥n esperada**
- **Filas**: 1140 (corresponden a 12 meses √ó 95 a√±os).  
- **Columnas**: 34 (incluyen ID de estaci√≥n, a√±o, mes y 31 d√≠as).

---

#### **Uso**
Garantiza que cada fichero procesado cumple con la estructura requerida antes de realizar an√°lisis adicionales.

---

### 3. **Validar valores enteros con logging**

```python
def validate_integers_with_logging(df, filename):
    for col in df.columns[1:]:
        for idx, value in enumerate(df[col]):
            try:
                int(value)
            except (ValueError, TypeError):
                logging.warning(
                    f"Invalid value '{value}' at row {idx}, column '{col}' in file '{filename}'"
                )
```
#### **Prop√≥sito**
Verifica que los valores en el fichero (excluyendo las cabeceras) sean de tipo `INT`.

---

#### **Detalles**

- **Entrada**:
  - `df`: DataFrame que contiene los datos del fichero.
  - `filename`: Nombre del fichero (para registro en el log).

- **Salida**:
  - No devuelve nada. Genera un warning si encuentra valores no v√°lidos.

---

#### **Validaci√≥n esperada**
- Todas las columnas (excepto la primera) deben contener valores enteros.

- **Errores manejados**:
  - `ValueError`: Valores no convertibles a entero.
  - `TypeError`: Tipos incompatibles con enteros.

---

#### **Uso**
Garantiza la consistencia del tipo de datos en los ficheros, ayudando a identificar valores inv√°lidos o inconsistencias antes de realizar an√°lisis adicionales.

#### **Una vez hemos validado los datos, podemos procesarlos y generar estadisticas.**

---

# 3. An√°lisis de Datos y Visualizaci√≥n

## 3.1 An√°lisis de Datos

Para realizar el an√°lisis, se ha dise√±ado un script en Python que permite procesar los datos y generar estad√≠sticas relevantes.

---

### Estad√≠sticas Generadas

Al ejecutar el script, se calculan las siguientes m√©tricas:

1. **Total de ficheros procesados**:  
   N√∫mero total de ficheros que se han analizado.

2. **Total de valores procesados**:  
   Cantidad de datos individuales procesados (por fila y columna).

3. **Total de valores nulos procesados**:  
   Cantidad de valores que est√°n vac√≠os o son inv√°lidos.

4. **Porcentaje de valores nulos**:  
   Relaci√≥n porcentual de valores nulos respecto al total.

---

### Fichero de Resultados

El script genera un √∫nico fichero que consolida la **media de todas las estaciones**, lo que permite obtener valores promedio a nivel nacional. La estructura del fichero es la siguiente:

```csv
YEAR    Mean                TotalPrecip         PctChange
2006    2.5051546391752577  897.80206185567
2007    1.288659793814433   461.5556701030927   -48.59048673277695
```

### Breakdown del script:

```python

for file_name in tqdm(lista_archivos, desc='Procesando archivos', unit='it'):
    file_path = os.path.join(Folder_path, file_name)

    try:
        # Leer el archivo CSV omitiendo las primeras dos filas
        df = pd.read_csv(file_path, skiprows=2, sep=r'\s+', engine='python', header=None)

        # Crear los nombres de los headers
        df.columns = column_name

        df.replace(-999, pd.NA, inplace=True)

        # Mostrar total de valores y valores nulos
        total_valores_procesados += df.size
        total_valores_nulos_procesados += df.isnull().sum().sum()

        # Cambiar el formato del DF a long format
        df_melted = df.melt(id_vars=['ID', 'YEAR', 'MONTH'], var_name='DAY', value_name='VALUE')
        # Ordenar los valores por a√±o y mes
        df_melted = df_melted.sort_values(by=['YEAR', 'MONTH'])

        # Contar el total de dias sin valores (null) (por a√±o)
        total_null_days = df_melted.groupby('YEAR')['VALUE'].apply(lambda x: x.isna().sum())

        # Contar el total de dias (por a√±o)
        total_count_days = df_melted.groupby('YEAR')['VALUE'].count()

        # Calcular el total de dias validos (con datos para hacer la divison luego) (por a√±o)
        total_valid_days = total_count_days - total_null_days
        # calcular la precipitacion total por a√±o i pasarla a metros por litro cuadrado (mm -> m^3)
        df_total_precip = df_melted.groupby('YEAR')['VALUE'].sum() / 10

        # Combrovar que si un a√±o es bisiesto se le a√±ade un dia
        for year in total_valid_days.index:
            if is_leap_year(year):
                total_valid_days.loc[year] += 1

        # Calcular la media de precipitacion por a√±o
        df_with_mean = df_total_precip / total_valid_days

        # Juntar los dos dataframes
        combined_df = pd.concat([df_with_mean.rename('Mean'), df_total_precip.rename('TotalPrecip')], axis=1)

        # Agregar los resultados al DataFrame agregado
        if aggregated_results.empty:
            aggregated_results = combined_df
        else:
            aggregated_results = aggregated_results.add(combined_df, fill_value=None)

        file_count += 1

    except Exception as e:
        logging.error(f'Error processing file {file_name}: {e}')
```
#### **Descripci√≥n**
Este bloque de c√≥digo procesa m√∫ltiples archivos de datos meteorol√≥gicos, calcula la precipitaci√≥n total y la media anual para cada archivo, y luego agrega los resultados en un DataFrame global. Durante el proceso, se realiza una serie de validaciones como la conversi√≥n de valores nulos, el ajuste para a√±os bisiestos, y la conversi√≥n de los datos a un formato m√°s manejable. Tambi√©n mantiene un registro de los valores procesados, los valores nulos y maneja errores que puedan ocurrir durante el procesamiento de los archivos. Finalmente, genera estad√≠sticas de precipitaci√≥n a nivel nacional a partir de todos los archivos procesados.

---

```python
aggregated_results['Mean'] = aggregated_results['Mean'].astype(int)

# Calcular la media de los valores agregados
mean_results = aggregated_results / file_count

# Ensure 'TotalPrecip' is in mean_results
mean_results['TotalPrecip'] = aggregated_results['TotalPrecip'] / file_count

# Calculate the percentage change of the 'TotalPrecip' column
mean_results['PctChange'] = mean_results['TotalPrecip'].pct_change() * 100

pct_nulos = (total_valores_nulos_procesados / total_valores_procesados) * 100

# Guardar el DataFrame de resultados medios en un archivo CSV
mean_results.to_csv('mean_output_with_stats.csv', index=True)

# Convert 'TotalPrecip' to numeric
aggregated_results['TotalPrecip'] = pd.to_numeric(aggregated_results['TotalPrecip'])

# Calculate top 5 driest and wettest years
top_5_driest = aggregated_results.nsmallest(5, 'TotalPrecip') / file_count
top_5_wettest = aggregated_results.nlargest(5, 'TotalPrecip') /file_count
```
#### **Descripci√≥n**
Este bloque de c√≥digo calcula la media de precipitaci√≥n anual, el cambio porcentual en la precipitaci√≥n, y el porcentaje de valores nulos en los datos procesados. Guarda los resultados en un archivo CSV y calcula los 5 a√±os m√°s secos y m√°s h√∫medos. Tambi√©n convierte las columnas relevantes a tipos de datos adecuados para los c√°lculos.

---

```python
# Write the results to the data.log file
with open('data.log', 'w') as log_file:
    log_file.write("=========================================\n")
    log_file.write("=                                       =\n")
    log_file.write("=         DATA ANALYSIS RESULT          =\n")
    log_file.write("=                                       =\n")
    log_file.write("=========================================\n\n")
    log_file.write(f"Date: {pd.Timestamp.now()}\n\n")
    log_file.write("=========================================\n")
    log_file.write(f"Total of files processed: {file_count}\n")
    log_file.write(f"Total of values processed: {total_valores_procesados}\n")
    log_file.write(f"Total of null or empty values: {total_valores_nulos_procesados}\n")
    log_file.write(f"Percentage of null over total data: {pct_nulos:.2f}%\n")
    log_file.write("=========================================\n\n")
    log_file.write("===== Top 5 Driest Years =====\n")
    log_file.write("=========================================\n")
    for year, row in top_5_driest.iterrows():
        log_file.write(f"Year {year}: Total: {round(row['TotalPrecip'], 2)} l/m¬≤, Mean: {round(row['TotalPrecip'] / file_count, 2)} l/m¬≤\n")
    log_file.write("=========================================\n\n")
    log_file.write("===== Top 5 Wettest Years =====\n")
    log_file.write("=========================================\n")
    for year, row in top_5_wettest.iterrows():
        log_file.write(f"Year {year}: Total: {round(row['TotalPrecip'], 2)} l/m¬≤, Mean: {round(row['TotalPrecip'] / file_count, 2)} l/m¬≤\n")
    log_file.write("=========================================\n\n")
    log_file.write("===== Dataframe with total and mean =====\n")
    log_file.write("=========================================\n\n")
    for year, row in aggregated_results.iterrows():
        log_file.write(f"Year {year}: Total: {round(row['TotalPrecip'], 2)} l/m¬≤, Mean: {round(row['TotalPrecip'] / file_count, 2)} l/m¬≤\n")

```
#### **Descripci√≥n**
Este es el bloque de codigo que escribe los resultados en el fichero data.log.

---
#### **Una vez hemos procesado los datos, podemos generar visualizaciones.**

---

## 3.2 Visualizacion de datos

Para visualizar los datos, lo hemos hecho de 2 formas. En local, con matplotlib y seaborn, y en la web, con javascript (exportando los datos a JSON).

Breakdown del script:

```python
df_results = pd.read_csv('.././data_analysis/mean_output_with_stats.csv')
df_top5dry = df_results.nsmallest(5, columns='TotalPrecip')
df_top5wet = df_results.nlargest(5, columns='TotalPrecip')
```
#### **Descripci√≥n**
Importamos el fichero con los resultados y creamos dos Dataframes, uno con los 5 a√±os mas secos y otro con los 5 a√±os mas humedos.
---
```python
plt.figure(figsize=(10, 6))
plt.bar(df_results.index, df_results['TotalPrecip'], label='Total Precipitation', color='b')
plt.bar(df_results.index, df_results['Mean'], label='Mean Precipitation', color='g')
plt.xlabel('Year')
plt.ylabel('Precipitation (l/m¬≤)')
plt.title('Total and Mean Precipitation Over Years')
plt.legend()
plt.grid(True)
plt.savefig("my_plot1.png", dpi=300, bbox_inches='tight')
plt.show()

# Visualize top 5 driest and wettest years in a single graph with different colors
fig, ax = plt.subplots(figsize=(10, 6))

years = list(df_top5dry.index) + list(df_top5wet.index)
values = list(df_top5dry['TotalPrecip']) + list(df_top5wet['TotalPrecip'])
colors = ['brown'] * len(df_top5dry) + ['blue'] * len(df_top5wet)

ax.bar(years, values, color=colors)
ax.set_title('Top 5 Driest and Wettest Years')
ax.set_xlabel('Year')
ax.set_ylabel('Total Precipitation (l/m¬≤)')

plt.tight_layout()
plt.savefig("my_plot2.png", dpi=300, bbox_inches='tight')
plt.show()

# Visualize the percentage change of the df_results DataFrame
plt.figure(figsize=(10, 6))
plt.plot(df_results.index, df_results['PctChange'], label='Percentage Change', color='r')
plt.xlabel('Year')
plt.ylabel('Percentage Change (%)')
plt.title('Percentage Change of Total Precipitation Over Years')
plt.legend()
plt.grid(True)
plt.savefig("my_plot3.png", dpi=300, bbox_inches='tight')
plt.show()
```
#### **Descripci√≥n**
Este es el bloque de codigo que genera las visualizaciones. La primera visualizacion muestra la precipitacion total y media a lo largo de los a√±os, la segunda visualizacion muestra los 5 a√±os mas secos y los 5 a√±os mas humedos y la tercera visualizacion muestra el porcentaje de cambio de la precipitacion total a lo largo de los a√±os.

---
```python
df_results.to_json('mean_results.json')
df_top5dry.to_json('top_5_driest.json')
df_top5wet.to_json('top_5_wettest.json')
```
#### **Descripci√≥n**
Este es el bloque de codigo que exporta los datos a JSON.

---

### En la siguiente imagen podemos ver las visualizaciones generadas:

#### Visualizacion 1
![Visualizaciones](./task_03/charts&graphs/my_plot1.png)

---
#### Visualizacion 2

![Visualizaciones](./task_03/charts&graphs/my_plot2.png)

---
#### Visualizacion 3
![Visualizaciones](./task_03/charts&graphs/my_plot3.png)

---

# 4. Publicacion de datos

Para publicar los datos, hemos creado una pagina web con HTML, CSS y JavaScript.

# 5. Reflexion

| Persona             | Qu√® he apr√®s?                                                                                      | Nivell |
|---------------------|---------------------------------------------------------------------------------------------------|--------|
| **Sharam Khan.**    | **Treball en grup**: he apr√®s a explicar les meves idees                                          | üü† For√ßa |
| **Sergio Lopez**    | **Treball en equip**, he apr√®s a consensuar la feina a fer                                        | üü† For√ßa |
| **Adri√†n Gonzalez** | **HTML i CSS**, per fi he ent√®s com crear una web i publicar-la                                   | üü¢ Molt |
| **Adri√† Manero**    | **Processament de dades**: he apr√®s com manipular fitxers molt grans                             | üî¥ Res |



# 6. Referencias

1. **OpenData AEMET**  
   Datos extra√≠dos de [OpenData AEMET](https://opendata.aemet.es/), utilizados para analizar la precipitaci√≥n en Espa√±a durante el periodo 2006-2100.  
   Modelo utilizado: Regresi√≥n en rejilla, **MIRCO5**, escenario **RCP6.0**.

2. **Librer√≠as y Tecnolog√≠as**  
   - **Python**: Procesamiento y an√°lisis de datos utilizando librer√≠as como `pandas`, `matplotlib` y `numpy`.  
   - **JavaScript**: Generaci√≥n de gr√°ficos interactivos mediante la librer√≠a [Chart.js](https://www.chartjs.org/).  
   - **HTML & CSS**: Estructura y dise√±o del sitio web, con un tema oscuro y tablas responsivas.

3. **Inspiraci√≥n del Dise√±o**  
   - Dise√±o inspirado en pr√°cticas de visualizaci√≥n de datos modernos y accesibles para un p√∫blico t√©cnico.  

4. **Documentaci√≥n Adicional**  
   - [Documentaci√≥n de Chart.js](https://www.chartjs.org/docs/latest/) para personalizaci√≥n de gr√°ficos.  
   - [Markdown Guide](https://www.markdownguide.org/) para la estructura y formato del README.
   - [Pandas Documentation](https://pandas.pydata.org/docs/) para la manipulaci√≥n de datos en Python.
   - [Tutorial Pandas](https://youtu.be/2uvysYbKdjM?si=AWb-gjNjECU1DuXq) para aprender a usar Pandas.
   - [Github copilot](https://copilot.github.com/) para la generaci√≥n de c√≥digo.
   
---